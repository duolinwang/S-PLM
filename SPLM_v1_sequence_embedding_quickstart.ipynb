{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "293abfc9",
      "metadata": {
        "id": "293abfc9"
      },
      "source": [
        "# **S-PLM v1: Sequence Embedding Quickstart**\n",
        "\n",
        "This notebook is a **concise usage example** of **S-PLM v1**. It demonstrates how to convert amino-acid **sequences** into model **embeddings**.\n",
        "\n",
        "* **Purpose:** Input protein sequences → output embeddings.\n",
        "* **Checkpoint:** An S-PLM v1 `.pth` checkpoint. If you have access, download from the provided [SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUf7oNxn1OpCse64KNleK3cBJ396ORTN338eRWRA4Q792A?e=Fgkuzk).\n",
        "* **Workflow:** load YAML config → build `SequenceRepresentation` → load checkpoint → tokenize with `batch_converter` → forward pass → save embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Setup**\n",
        "\n",
        "We **recommend** using an NVIDIA **A100** in Colab; other GPUs/CPU will work but may be slower or run into memory limits.\n"
      ],
      "metadata": {
        "id": "J0Zx6LDn_ZEu"
      },
      "id": "J0Zx6LDn_ZEu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf665391",
      "metadata": {
        "id": "cf665391"
      },
      "outputs": [],
      "source": [
        "# Clone S-PLM\n",
        "!git clone -q https://github.com/duolinwang/S-PLM /content/S-PLM\n",
        "\n",
        "# Install minimal deps\n",
        "!pip install 'git+https://github.com/facebookresearch/esm.git' -q\n",
        "!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup' -q\n",
        "# for downstream tasks only\n",
        "!pip install torchmetrics -q\n",
        "!pip uninstall -y accelerate -q\n",
        "!pip install accelerate==0.34.2 -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efac75f",
      "metadata": {
        "id": "3efac75f"
      },
      "source": [
        "### **Prepare Checkpoint**\n",
        "\n",
        "1. **Download the model** from the provided **[SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUf7oNxn1OpCse64KNleK3cBJ396ORTN338eRWRA4Q792A?e=Fgkuzk)** to your local machine.\n",
        "2. **Upload to your Colab runtime** (Files pane → Upload to session storage), then set:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/checkpoint_0520000.pth\""
      ],
      "metadata": {
        "id": "U-SK-aJNDnXy"
      },
      "id": "U-SK-aJNDnXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Faster option (recommended):** Mount Google Drive and copy the checkpoint from Drive into the Colab runtime.\n"
      ],
      "metadata": {
        "id": "YLdBSzFSDqcc"
      },
      "id": "YLdBSzFSDqcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba12f7e",
      "metadata": {
        "id": "1ba12f7e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "import os, shutil\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.copy(\"/content/drive/MyDrive/checkpoint_0520000.pth\",\n",
        "            \"/content/checkpoint_0520000.pth\")\n",
        "CHECKPOINT_PATH = \"/content/checkpoint_0520000.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070fd6a9",
      "metadata": {
        "id": "070fd6a9"
      },
      "source": [
        "### **Minimal embedding code**\n",
        "\n",
        "* Load the YAML config\n",
        "* Initialize `SequenceRepresentation`\n",
        "* Load the checkpoint\n",
        "* Tokenize sequences with `batch_converter`\n",
        "* Run a forward pass to obtain `protein_representation` and `residue_representation`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69903c0d",
      "metadata": {
        "id": "69903c0d"
      },
      "outputs": [],
      "source": [
        "assert CHECKPOINT_PATH, \"Set CHECKPOINT_PATH to your .pth (downloaded or uploaded).\"\n",
        "\n",
        "import sys, os, torch, yaml, numpy as np\n",
        "sys.path.insert(0, \"/content/S-PLM\")\n",
        "import yaml\n",
        "from utils import load_configs, load_checkpoints_only\n",
        "from model import SequenceRepresentation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the configuration file\n",
        "config_path = \"/content/S-PLM/configs/representation_config.yaml\"\n",
        "with open(config_path) as f:\n",
        "    dict_config = yaml.full_load(f)\n",
        "configs = load_configs(dict_config)\n",
        "\n",
        "# Create the model using the configuration file\n",
        "model = SequenceRepresentation(logging=None, configs=configs).to(device)\n",
        "model.eval()\n",
        "load_checkpoints_only(CHECKPOINT_PATH, model)\n",
        "\n",
        "# Create a list of protein sequences (edit these)\n",
        "sequences = [\n",
        "    \"MHHHHHHSSGVDLGTENLYFQSNAMDFPQQLEA\",\n",
        "    \"CVKQANQALSRFIAPLPFQNTPVVE\",\n",
        "    \"TMQYGALLGGKRLR\",\n",
        "]\n",
        "\n",
        "esm2_seq = [(i, seq) for i, seq in enumerate(sequences)]\n",
        "batch_labels, batch_strs, batch_tokens = model.batch_converter(esm2_seq)\n",
        "batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    protein_representation, residue_representation, mask = model(batch_tokens)\n",
        "\n",
        "protein_embedding = protein_representation.detach().cpu().numpy()  # [N, D]\n",
        "print(\"Protein embeddings shape:\", protein_embedding.shape)\n",
        "print(\"Residue embeddings tensor shape (with padding):\", residue_representation.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downstream Tasks**\n",
        "\n",
        "S-PLM v1 provides lightweight tuning code for **supervised downstream tasks** including **EC**, **GO**, **fold**, **ER**, and **secondary structure (SS)** prediction. You set hyperparameters in the corresponding config file and launch the task-specific training script with your pretrained checkpoint.\n",
        "\n",
        "**Task scripts:** `train_ec.py`, `train_er.py`, `train_fold.py`, `train_go.py`, `train_ss.py`\n",
        "**Configs:** `configs/config_{task}.yaml` (multiple examples provided)\n",
        "\n",
        "S-PLM supports **fine-tuning top layers, Adapter Tuning, and LoRA**; pick the variant by choosing the matching config.\n",
        "\n",
        "\n",
        "**Command template**\n",
        "\n",
        "```bash\n",
        "accelerate launch train_{task}.py \\\n",
        "  --config_path configs/<config_name>.yaml \\\n",
        "  --resume_path checkpoint_0520000.pth\n",
        "```\n",
        "\n",
        "**Examples**\n",
        "\n",
        "```bash\n",
        "# GO (BP / CC / MF)\n",
        "!python train_go.py --config_path configs/bp_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "!python train_go.py --config_path configs/cc_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "!python train_go.py --config_path configs/mf_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "\n",
        "# Fold classification\n",
        "!python train_fold.py --config_path configs/fold_config_adapterH_finetune.yaml --resume_path checkpoint_0520000.pth\n",
        "\n",
        "# Secondary structure\n",
        "!python train_ss.py --config_path configs/ss_config_adapterH_finetune.yaml --resume_path checkpoint_0520000.pth\n",
        "```"
      ],
      "metadata": {
        "id": "SGrIOh9BZUyK"
      },
      "id": "SGrIOh9BZUyK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Colab GPUs can be limited; to run in Colab, **reduce batch size** (see `configs/bp_config_adapterH_adapterH_ColabA10040G.yaml`)."
      ],
      "metadata": {
        "id": "J71fNpW3l6JK"
      },
      "id": "J71fNpW3l6JK"
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/S-PLM /S-PLM\n",
        "!python /content/S-PLM/train_go.py --config_path /content/S-PLM/configs/bp_config_adapterH_adapterH_ColabA10040G.yaml --resume_path /content/checkpoint_0520000.pth"
      ],
      "metadata": {
        "id": "YHX9LLd-ZUK7"
      },
      "id": "YHX9LLd-ZUK7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J0Zx6LDn_ZEu",
        "3efac75f",
        "070fd6a9"
      ],
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}