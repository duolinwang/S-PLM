{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "293abfc9",
      "metadata": {
        "id": "293abfc9"
      },
      "source": [
        "# **S-PLM v1: Sequence Embedding Quickstart**\n",
        "\n",
        "This notebook is a **usage example** of **S-PLM v1**. It demonstrates how to convert amino-acid **sequences** into model **embeddings**, and how to run **downstream training/evaluation**.\n",
        "\n",
        "* **Purpose:** Produce protein/residue embeddings; fine-tune/evaluate downstream tasks (GO/EC/fold/ER/SS).\n",
        "* **Checkpoint:** An S-PLM v1 `.pth` checkpoint. Download from the provided [SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUf7oNxn1OpCse64KNleK3cBJ396ORTN338eRWRA4Q792A?e=Fgkuzk).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Setup**\n",
        "\n",
        "We **recommend** using an NVIDIA **A100** in Colab; other GPUs/CPU will work but may be slower or run into memory limits.\n"
      ],
      "metadata": {
        "id": "J0Zx6LDn_ZEu"
      },
      "id": "J0Zx6LDn_ZEu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf665391",
      "metadata": {
        "id": "cf665391"
      },
      "outputs": [],
      "source": [
        "# Clone S-PLM\n",
        "!git clone -q https://github.com/duolinwang/S-PLM /content/S-PLM\n",
        "\n",
        "# Install minimal deps\n",
        "!pip install 'git+https://github.com/facebookresearch/esm.git' -q\n",
        "!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup' -q\n",
        "# for downstream tasks only\n",
        "!pip install torchmetrics -q\n",
        "!pip uninstall -y accelerate -q\n",
        "!pip install accelerate==0.34.2 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "id": "CojB3t9X6Xh5"
      },
      "id": "CojB3t9X6Xh5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3efac75f",
      "metadata": {
        "id": "3efac75f"
      },
      "source": [
        "### **Prepare Checkpoint**\n",
        "\n",
        "1. **Download the model** from the provided **[SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUf7oNxn1OpCse64KNleK3cBJ396ORTN338eRWRA4Q792A?e=Fgkuzk)** to your local machine.\n",
        "2. **Upload to your Colab runtime** (Files pane → Upload to session storage), then set:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/checkpoint_0520000.pth\""
      ],
      "metadata": {
        "id": "U-SK-aJNDnXy"
      },
      "id": "U-SK-aJNDnXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Faster option (recommended):** Mount Google Drive and copy the checkpoint from Drive into the Colab runtime.\n"
      ],
      "metadata": {
        "id": "YLdBSzFSDqcc"
      },
      "id": "YLdBSzFSDqcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba12f7e",
      "metadata": {
        "id": "1ba12f7e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "import os, shutil\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.copy(\"/content/drive/MyDrive/checkpoint_0520000.pth\",\n",
        "            \"/content/checkpoint_0520000.pth\")\n",
        "CHECKPOINT_PATH = \"/content/checkpoint_0520000.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070fd6a9",
      "metadata": {
        "id": "070fd6a9"
      },
      "source": [
        "### **Minimal embedding code**\n",
        "Use GVP model to generate embeddings from FASTA sequences, with optional truncation and residue-level outputs.\n",
        "\n",
        "* **Standard run:** produces **protein-level** embeddings from `.fasta` to `.pkl`\n",
        "* **Truncated run:** sets `--truncate_inference 1 --max_length_inference 1022` to handle long sequences\n",
        "\n",
        "* **Residue-level run:** adds `--residue_level`\n",
        "\n",
        "**Inputs:** `--input_seq` (FASTA), `--config_path`, `--checkpoint_path`.\n",
        "\n",
        "**Outputs:** pickled embeddings in the working directory (per protein or per residue, depending on flags).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/S-PLM')\n",
        "\n",
        "# standard run\n",
        "!python generate_seq_embedding.py --input_seq /content/S-PLM/sample_fasta/protein.fasta \\\n",
        "  --config_path /content/S-PLM/configs/SPLM1_representation_config.yaml \\\n",
        "  --checkpoint_path /content/checkpoint_0520000.pth \\\n",
        "  --result_path ./"
      ],
      "metadata": {
        "id": "uDY0uezA4hDi"
      },
      "id": "uDY0uezA4hDi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Minimal embedding code (Python script)**\n",
        "* Load the YAML config\n",
        "* Initialize `SequenceRepresentation`\n",
        "* Load the checkpoint\n",
        "* Tokenize sequences with `batch_converter`\n",
        "* Run a forward pass to obtain `protein_representation` and `residue_representation`"
      ],
      "metadata": {
        "id": "cSTKR0V17sfb"
      },
      "id": "cSTKR0V17sfb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69903c0d",
      "metadata": {
        "id": "69903c0d"
      },
      "outputs": [],
      "source": [
        "assert CHECKPOINT_PATH, \"Set CHECKPOINT_PATH to your .pth (downloaded or uploaded).\"\n",
        "\n",
        "import sys, os, torch, yaml, numpy as np\n",
        "sys.path.insert(0, \"/content/S-PLM\")\n",
        "import yaml\n",
        "from utils import load_configs, load_checkpoints_only\n",
        "from model import SequenceRepresentation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the configuration file\n",
        "config_path = \"/content/S-PLM/configs/representation_config.yaml\"\n",
        "with open(config_path) as f:\n",
        "    dict_config = yaml.full_load(f)\n",
        "configs = load_configs(dict_config)\n",
        "\n",
        "# Create the model using the configuration file\n",
        "model = SequenceRepresentation(logging=None, configs=configs).to(device)\n",
        "model.eval()\n",
        "load_checkpoints_only(CHECKPOINT_PATH, model)\n",
        "\n",
        "# Create a list of protein sequences (edit these)\n",
        "sequences = [\n",
        "    \"MHHHHHHSSGVDLGTENLYFQSNAMDFPQQLEA\",\n",
        "    \"CVKQANQALSRFIAPLPFQNTPVVE\",\n",
        "    \"TMQYGALLGGKRLR\",\n",
        "]\n",
        "\n",
        "esm2_seq = [(i, seq) for i, seq in enumerate(sequences)]\n",
        "batch_labels, batch_strs, batch_tokens = model.batch_converter(esm2_seq)\n",
        "batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    protein_representation, residue_representation, mask = model(batch_tokens)\n",
        "\n",
        "protein_embedding = protein_representation.detach().cpu().numpy()  # [N, D]\n",
        "print(\"Protein embeddings shape:\", protein_embedding.shape)\n",
        "print(\"Residue embeddings tensor shape (with padding):\", residue_representation.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **General clustering evaluation (CATH / Kinase)**\n",
        "\n",
        "We evaluate sequence embedding quality using clustering-based analyses. We report both visualizations (t-SNE scatter plots) and quantitative metrics (Calinski–Harabasz, ARI, silhouette).\n",
        "\n",
        "\n",
        "### Inputs\n",
        "\n",
        "* `checkpoint_path`: path to the pretrained model checkpoint (`.pth`)\n",
        "* `config_path`: path to the YAML config used for the checkpoint\n",
        "* Path to the evaluation dataset (format depends on `task`)\n",
        "\n",
        "  * `cath_seq`: CATH FASTA file with CATH codes in headers (e.g., `1.10.10.2080|cath|...`)\n",
        "  * `kinase_seq`: TSV file containing kinase metadata and sequences (e.g., `Kinase_group`, `Kinase_domain`)\n",
        "\n",
        "### What it does\n",
        "* **Computes embeddings** for all samples in the dataset.\n",
        "* **Runs clustering evaluation** at one or more label granularities (e.g., CATH Class / Architecture / Fold, or Kinase Group).\n",
        "* **Generates visualizations**:\n",
        "\n",
        "  * Projects embeddings to 2D using t-SNE and saves scatter plots colored by ground-truth labels.\n",
        "* **Computes clustering metrics**:\n",
        "\n",
        "  * Calinski–Harabasz score (full space and t-SNE 2D)\n",
        "  * Adjusted Rand Index (ARI) using k-means on the t-SNE space\n",
        "  * Silhouette score in the full embedding space\n",
        "* **Saves outputs**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aYyy-nOufMv7"
      },
      "id": "aYyy-nOufMv7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/S-PLM')\n",
        "\n",
        "!python cath_with_seq.py \\\n",
        "  --cath_seq ./dataset/Rep_subfamily_basedon_S40pdb.fa \\\n",
        "  --checkpoint_path /content/checkpoint_0520000.pth \\\n",
        "  --config_path /content/S-PLM/configs/SPLM1_representation_config.yaml"
      ],
      "metadata": {
        "id": "x_Jz0hbQfoTu"
      },
      "id": "x_Jz0hbQfoTu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "paths = [\n",
        "    \"/content/S-PLM/configs/SPLM1_representation_config/CATH_test_release_seq/step_0_CATH_1.png\",\n",
        "    \"/content/S-PLM/configs/SPLM1_representation_config/CATH_test_release_seq/step_0_CATH_2.png\",\n",
        "    \"/content/S-PLM/configs/SPLM1_representation_config/CATH_test_release_seq/step_0_CATH_3.png\",\n",
        "]\n",
        "\n",
        "imgs = [Image.open(p) for p in paths]\n",
        "\n",
        "total_width = sum(im.width for im in imgs)\n",
        "max_height = max(im.height for im in imgs)\n",
        "\n",
        "new_img = Image.new(\"RGB\", (total_width, max_height), (255, 255, 255))\n",
        "x = 0\n",
        "for im in imgs:\n",
        "    new_img.paste(im, (x, 0))\n",
        "    x += im.width\n",
        "\n",
        "\n",
        "dpi = 800\n",
        "plt.figure(figsize=(total_width / dpi, max_height / dpi), dpi=dpi)\n",
        "plt.imshow(new_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OHmqayXwo3Pf"
      },
      "id": "OHmqayXwo3Pf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/S-PLM')\n",
        "\n",
        "!python kinase_with_seq.py \\\n",
        "  --kinase_path ./dataset/GPS5.0_homo_hasPK_with_kinasedomain.txt \\\n",
        "  --checkpoint_path /content/checkpoint_0520000.pth \\\n",
        "  --config_path /content/S-PLM/configs/SPLM1_representation_config.yaml"
      ],
      "metadata": {
        "id": "EvZ0mRSko9_j"
      },
      "id": "EvZ0mRSko9_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open(\"/content/S-PLM/configs/SPLM1_representation_config/Kinase_test_release_seq/step_0_kinase.png\")\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y7DId97GpQuw"
      },
      "id": "Y7DId97GpQuw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downstream Tasks**\n",
        "\n",
        "S-PLM v1 provides lightweight tuning code for **supervised downstream tasks** including **EC**, **GO**, **fold**, **ER**, and **secondary structure (SS)** prediction. You set hyperparameters in the corresponding config file and launch the task-specific training script with your pretrained checkpoint.\n",
        "\n",
        "**Task scripts:** `train_ec.py`, `train_er.py`, `train_fold.py`, `train_go.py`, `train_ss.py`\n",
        "**Configs:** `configs/config_{task}.yaml` (multiple examples provided)\n",
        "\n",
        "S-PLM supports **fine-tuning top layers, Adapter Tuning, and LoRA**; pick the variant by choosing the matching config.\n",
        "\n",
        "\n",
        "**Command template**\n",
        "\n",
        "```bash\n",
        "accelerate launch train_{task}.py \\\n",
        "  --config_path configs/<config_name>.yaml \\\n",
        "  --resume_path checkpoint_0520000.pth\n",
        "```\n",
        "\n",
        "**Examples**\n",
        "\n",
        "```bash\n",
        "# GO (BP / CC / MF)\n",
        "!python train_go.py --config_path configs/bp_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "!python train_go.py --config_path configs/cc_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "!python train_go.py --config_path configs/mf_config_adapterH_adapterH.yaml --resume_path checkpoint_0520000.pth\n",
        "\n",
        "# Fold classification\n",
        "!python train_fold.py --config_path configs/fold_config_adapterH_finetune.yaml --resume_path checkpoint_0520000.pth\n",
        "\n",
        "# Secondary structure\n",
        "!python train_ss.py --config_path configs/ss_config_adapterH_finetune.yaml --resume_path checkpoint_0520000.pth\n",
        "```"
      ],
      "metadata": {
        "id": "SGrIOh9BZUyK"
      },
      "id": "SGrIOh9BZUyK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Colab GPUs can be limited; to run in Colab, **reduce batch size** (see `configs/bp_config_adapterH_adapterH_ColabA10040G.yaml`)."
      ],
      "metadata": {
        "id": "J71fNpW3l6JK"
      },
      "id": "J71fNpW3l6JK"
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/S-PLM /S-PLM\n",
        "!python /content/S-PLM/train_go.py --config_path /content/S-PLM/configs/bp_config_adapterH_adapterH_ColabA10040G.yaml --resume_path /content/checkpoint_0520000.pth"
      ],
      "metadata": {
        "id": "YHX9LLd-ZUK7"
      },
      "id": "YHX9LLd-ZUK7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}